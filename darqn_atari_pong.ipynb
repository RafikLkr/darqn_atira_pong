{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319c3d80",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99737453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# darqn_atari.py\n",
    "# üéÆ DARQN minimal avec CNN + CBAM + LSTM sur Pong\n",
    "# Objectif : Apprendre √† un agent √† jouer √† Pong depuis les images brutes\n",
    "# ‚Äî Entra√Ænement sans affichage\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers, losses, ops\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n",
    "import ale_py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af3285e",
   "metadata": {},
   "source": [
    "ENVIRONNEMENT : PONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enregistre les environnements Atari dans Gymnasium\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# Param√®tres principaux\n",
    "\n",
    "ENV_ID = \"ALE/Pong-v5\"      # Environnement Atari : Pong\n",
    "SEQ_LEN = 4                 # Nombre de frames empil√©es (m√©moire temporelle)\n",
    "GAMMA = 0.99                # Facteur de discount (importance des r√©compenses futures)\n",
    "LEARNING_RATE = 1e-4\n",
    "EPSILON = 1.0               # Probabilit√© initiale d‚Äôexplorer\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 100_000\n",
    "EPISODES = 500\n",
    "TARGET_UPDATE_EVERY = 10    # Tous les 10 √©pisodes, on synchronise le r√©seau cible\n",
    "\n",
    "# Pr√©paration de l‚Äôenvironnement Atari\n",
    "\n",
    "def make_env():\n",
    "    \"\"\"\n",
    "    Cr√©e un environnement Pong en niveaux de gris (84x84),\n",
    "    sans frameskip suppl√©mentaire, et avec empilement de 4 images cons√©cutives.\n",
    "    \"\"\"\n",
    "    env = gym.make(ENV_ID, render_mode=\"rgb_array\")\n",
    "\n",
    "    # AtariPreprocessing :\n",
    "    # - Convertit les images en 84x84\n",
    "    # - Passe en niveaux de gris (plus simple que 3 canaux RGB)\n",
    "    # - G√®re les no-op al√©atoires (pour diversifier les d√©buts de partie)\n",
    "    env = AtariPreprocessing(\n",
    "        env,\n",
    "        screen_size=84,\n",
    "        grayscale_obs=True,\n",
    "        frame_skip=1,          # On d√©sactive le saut de frames du wrapper\n",
    "        noop_max=30,\n",
    "        terminal_on_life_loss=False,\n",
    "        scale_obs=False,\n",
    "    )\n",
    "\n",
    "    # FrameStackObservation :\n",
    "    # - Empile les 4 derni√®res frames (T=4)\n",
    "    #   ‚Üí Cela donne √† l‚Äôagent une perception du mouvement\n",
    "    env = FrameStackObservation(env, stack_size=SEQ_LEN, padding_type=\"reset\")\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "def obs_to_seq(obs):\n",
    "    \"\"\"Convertit l‚Äôobservation (stack de 4 frames) en tenseur normalis√© [0,1].\"\"\"\n",
    "    arr = np.array(obs, dtype=np.float32) / 255.0  # Normalisation\n",
    "    if arr.ndim == 3:\n",
    "        arr = arr[..., np.newaxis]  # (T,84,84,1)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e31a7d",
   "metadata": {},
   "source": [
    "CBAM (Convolutional Block Attention Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CBAM (Convolutional Block Attention Module)\n",
    "# ============================================================\n",
    "\n",
    "def cbam_block(x, reduction=16, spatial_kernel=7):\n",
    "    \"\"\"\n",
    "    CBAM = Channel and Spatial Attention Module.\n",
    "    Permet au r√©seau de se concentrer sur les zones et canaux importants.\n",
    "    - \"Channel attention\" : apprend quels filtres sont utiles.\n",
    "    - \"Spatial attention\" : apprend o√π regarder dans l‚Äôimage.\n",
    "    \"\"\"\n",
    "    ch = x.shape[-1]\n",
    "\n",
    "    # --- Attention par canal ---\n",
    "    # Moyenne et max globales ‚Üí donnent une id√©e de l‚Äôimportance de chaque canal\n",
    "    gap = layers.GlobalAveragePooling2D(keepdims=True)(x)\n",
    "    gmp = layers.GlobalMaxPooling2D(keepdims=True)(x)\n",
    "    mid = int(max(1, ch // reduction))\n",
    "\n",
    "    # Petit MLP partag√© (2 convs 1x1)\n",
    "    mlp = models.Sequential([\n",
    "        layers.Conv2D(mid, 1, activation=\"relu\", padding=\"same\"),\n",
    "        layers.Conv2D(ch,  1, activation=\"sigmoid\", padding=\"same\"),\n",
    "    ])\n",
    "\n",
    "    # Combine GAP + GMP et applique le masque de canal\n",
    "    ca = layers.Add()([mlp(gap), mlp(gmp)])\n",
    "    x = layers.Multiply()([x, ca])\n",
    "\n",
    "    # --- Attention spatiale ---\n",
    "    # On calcule la moyenne et le max des canaux ‚Üí on fusionne (concat)\n",
    "    avg_sp = layers.Lambda(lambda t: ops.mean(t, axis=-1, keepdims=True))(x)\n",
    "    max_sp = layers.Lambda(lambda t: ops.max(t, axis=-1,  keepdims=True))(x)\n",
    "    concat = layers.Concatenate(axis=-1)([avg_sp, max_sp])\n",
    "    # Convolution 7x7 ‚Üí d√©tecte les zones spatialement pertinentes\n",
    "    sa = layers.Conv2D(1, spatial_kernel, padding=\"same\", activation=\"sigmoid\")(concat)\n",
    "    x = layers.Multiply()([x, sa])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2077e8",
   "metadata": {},
   "source": [
    "MODELE D'ENTRAINEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2002c84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rafle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üß© R√©seau Q : CNN + CBAM + LSTM\n",
    "\n",
    "def create_q_model(input_shape=(SEQ_LEN, 84, 84, 1), n_actions=n_actions):\n",
    "    \"\"\"\n",
    "    R√©seau Q avec traitement spatio-temporel :\n",
    "    - CNN : extrait les caract√©ristiques visuelles.\n",
    "    - CBAM : module d‚Äôattention, renforce les r√©gions/canaux importants.\n",
    "    - LSTM : int√®gre la dimension temporelle (mouvements, dynamiques).\n",
    "    - Dense : estime les valeurs Q pour chaque action possible.\n",
    "    \"\"\"\n",
    "\n",
    "    seq_in = layers.Input(shape=input_shape)  # (T,84,84,1)\n",
    "\n",
    "    # --- Bloc CNN (analyse spatiale) ---\n",
    "    # Convolutions pour extraire les motifs visuels (balle, raquettes‚Ä¶)\n",
    "    inp = layers.Input(shape=(84, 84, 1))\n",
    "    y = layers.Conv2D(32, 8, strides=4, activation=\"relu\", padding=\"same\")(inp)\n",
    "    y = layers.Conv2D(64, 4, strides=2, activation=\"relu\", padding=\"same\")(y)\n",
    "    y = layers.Conv2D(64, 3, strides=1, activation=\"relu\", padding=\"same\")(y)\n",
    "\n",
    "    # --- CBAM (focalisation sur les bonnes zones) ---\n",
    "    y = cbam_block(y)\n",
    "\n",
    "    # --- Flatten + Dense ---\n",
    "    # On obtient un vecteur compact de 256 dimensions par frame\n",
    "    y = layers.Flatten()(y)\n",
    "    y = layers.Dense(256, activation=\"relu\")(y)\n",
    "    frame_encoder = models.Model(inp, y, name=\"frame_encoder_cbam\")\n",
    "\n",
    "    # --- TimeDistributed ---\n",
    "    # On applique le CNN+CBAM sur chaque frame de la s√©quence temporelle (T=4)\n",
    "    x = layers.TimeDistributed(frame_encoder)(seq_in)  # (B,T,256)\n",
    "\n",
    "    # --- LSTM ---\n",
    "    # LSTM traite la s√©quence de 4 vecteurs 256D pour capturer le mouvement.\n",
    "    # ‚Üí permet d‚Äôanticiper la trajectoire de la balle.\n",
    "    x = layers.LSTM(128, return_sequences=False)(x)  # (B,128)\n",
    "\n",
    "    # --- Dense finale ---\n",
    "    # Calcule les valeurs Q(s,a) pour chaque action.\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    q_out = layers.Dense(n_actions, activation=\"linear\")(x)\n",
    "\n",
    "    # --- Compilation ---\n",
    "    # Huber loss : plus stable que MSE (moins sensible aux outliers)\n",
    "    model = models.Model(seq_in, q_out, name=\"Q_CBAM_LSTM\")\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=losses.Huber()\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# On cr√©e le r√©seau principal (Q) et le r√©seau cible (target)\n",
    "q_model = create_q_model()\n",
    "target_model = create_q_model()\n",
    "target_model.set_weights(q_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78286854",
   "metadata": {},
   "source": [
    "MEMOIRE D'EXPERIENCE : STORE_TRANSITION Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b49bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß± M√©moire d‚Äôexp√©rience (Experience Replay)\n",
    "\n",
    "memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "def store_transition(state_seq, action, reward, next_state_seq, done):\n",
    "    \"\"\"\n",
    "    Stocke une exp√©rience (transition) dans la m√©moire :\n",
    "    (s, a, r, s', done)\n",
    "    \"\"\"\n",
    "    memory.append((\n",
    "        state_seq.astype(np.float32),\n",
    "        int(action),\n",
    "        float(np.sign(reward)),  # on clip les r√©compenses (-1, 0, +1)\n",
    "        next_state_seq.astype(np.float32),\n",
    "        bool(done),\n",
    "    ))\n",
    "\n",
    "def sample_batch():\n",
    "    \"\"\"Renvoie un batch al√©atoire pour l'entra√Ænement.\"\"\"\n",
    "    batch = random.sample(memory, BATCH_SIZE)\n",
    "    s, a, r, s2, d = map(np.asarray, zip(*batch))\n",
    "    return np.array(s), np.array(a), np.array(r), np.array(s2), np.array(d, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a53db8",
   "metadata": {},
   "source": [
    "FONCTION D'EXPLORATION EPSILON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57d3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Politique epsilon-greedy\n",
    "\n",
    "def epsilon_greedy_policy(state_seq, epsilon):\n",
    "    \"\"\"\n",
    "    Choisit une action :\n",
    "    - al√©atoire (exploration) avec probabilit√© epsilon\n",
    "    - ou la meilleure selon Q (exploitation)\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_actions)\n",
    "    q_values = q_model.predict(state_seq[np.newaxis, ...], verbose=0)[0]\n",
    "    return int(np.argmax(q_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc50a24",
   "metadata": {},
   "source": [
    "FONCTION D'ENTRAINEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17eda3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'entra√Ænement\n",
    "\n",
    "def train_step():\n",
    "    \"\"\"Un pas d'apprentissage du DQN.\"\"\"\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    s, a, r, s2, d = sample_batch()\n",
    "\n",
    "    # Q(s',a') du mod√®le cible\n",
    "    next_q = target_model.predict(s2, verbose=0)\n",
    "    max_next_q = np.max(next_q, axis=1)\n",
    "    targets = r + (1.0 - d) * GAMMA * max_next_q\n",
    "\n",
    "    # Q(s,a) courant du mod√®le principal\n",
    "    q_curr = q_model.predict(s, verbose=0)\n",
    "    for i in range(BATCH_SIZE):\n",
    "        q_curr[i, a[i]] = targets[i]\n",
    "\n",
    "    q_model.fit(s, q_curr, verbose=0, batch_size=BATCH_SIZE, epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb219da3",
   "metadata": {},
   "source": [
    "ON LANCE LE MODELE D'ENTRAINEMENT : AVEC UNE PETITE ANALYSE A CHAQUE EPOQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265bb83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle d'entra√Ænement\n",
    "\n",
    "reward_history = []\n",
    "epsilon = EPSILON\n",
    "\n",
    "for episode in range(1, EPISODES + 1):\n",
    "    obs, info = env.reset()\n",
    "    state_seq = obs_to_seq(obs)\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        # Choisir une action selon epsilon-greedy\n",
    "        action = epsilon_greedy_policy(state_seq, epsilon)\n",
    "\n",
    "        # Ex√©cuter l'action\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state_seq = obs_to_seq(next_obs)\n",
    "\n",
    "        # Sauvegarder la transition\n",
    "        store_transition(state_seq, action, reward, next_state_seq, done)\n",
    "        total_reward += reward\n",
    "        state_seq = next_state_seq\n",
    "\n",
    "        # Mise √† jour du r√©seau\n",
    "        train_step()\n",
    "\n",
    "    # Mise √† jour du r√©seau cible et de l‚Äôexploration\n",
    "    epsilon = max(EPSILON_MIN, epsilon * EPSILON_DECAY)\n",
    "    if episode % TARGET_UPDATE_EVERY == 0:\n",
    "        target_model.set_weights(q_model.get_weights())\n",
    "\n",
    "    reward_history.append(total_reward)\n",
    "    avg100 = np.mean(reward_history[-100:])\n",
    "    print(f\"Ep {episode:5d} | eps={epsilon:.3f} | EpR={total_reward:+.1f} | Avg100={avg100:+.1f} | Memory={len(memory)}\")\n",
    "\n",
    "env.close()\n",
    "print(\"‚úÖ Training finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77f23c",
   "metadata": {},
   "source": [
    "--- FINI ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
