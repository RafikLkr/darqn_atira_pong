🧠 Deep Q-Learning with CNN + CBAM + LSTM on Pong 🎮
🎯 Objectif du projet

L’objectif de ce projet est de développer un agent intelligent capable d’apprendre à jouer au jeu Pong (Atari) en observant uniquement les images brutes de l’écran.
L’agent utilise un réseau de neurones profond (DQN) combiné à :

🧩 CBAM (Convolutional Block Attention Module) pour concentrer son attention sur les zones et canaux d’image les plus pertinents (balle, raquette),

🔁 LSTM (Long Short-Term Memory) pour mémoriser la dynamique temporelle du jeu (vitesse et trajectoire de la balle).

🕹️ Présentation du jeu Pong

Pong est un jeu d’arcade classique simulant une partie de tennis de table.

Le joueur contrôle une raquette sur le côté gauche de l’écran pour renvoyer une balle.

Le but est d’empêcher la balle de sortir de son côté tout en essayant de marquer sur l’adversaire (droite).

La récompense dans ce projet :

+1 pour un point gagné,

-1 pour un point perdu,

0 sinon.

Ce cadre est parfait pour le renforcement par apprentissage, car le but de l’agent est de maximiser la somme des récompenses cumulées.

⚙️ Architecture du modèle

L’architecture implémente un Deep Q-Network (DQN) enrichi par des modules d’attention et une mémoire temporelle :

Image (84x84x1) × 4 frames consécutives
     │
     ▼
Conv2D (32 filtres, 8x8, stride 4)   → Détection grossière des objets
Conv2D (64 filtres, 4x4, stride 2)   → Motifs intermédiaires
Conv2D (64 filtres, 3x3, stride 1)   → Détails fins
CBAM (Channel + Spatial Attention)   → Focus sur zones et canaux utiles
Flatten + Dense(256, ReLU)           → Encodage visuel par frame
TimeDistributed(...) × 4 frames      → Séquence temporelle
LSTM(128)                            → Apprentissage de la dynamique (vitesse, direction)
Dense(128, ReLU)
Dense(nb_actions, Linear)            → Valeurs Q pour chaque action


Perte : Huber Loss (plus stable que MSE)
Optimiseur : Adam
Réseau cible : mis à jour tous les 10 épisodes
Stratégie : ε-greedy avec décroissance progressive de ε

📊 Résultats d’apprentissage
🟠 Début d’entraînement

Épisodes 1 à 20 :

Les récompenses sont constantes autour de −21, correspondant à une défaite immédiate à chaque partie.

La moyenne mobile des 100 derniers épisodes (Avg100) reste stable à −21.

L’agent explore aléatoirement sans encore apprendre de stratégie.

🧩 Interprétation :

À ce stade, l’agent joue sans logique : il ne sait ni suivre la balle, ni anticiper les rebonds.
Le réseau est en phase d’exploration et le Q-learning n’a pas encore convergé.

🟢 Milieu/Fin d’entraînement

Épisodes 100 à 120 :

Les récompenses passent de −21 à environ −12 sur certains épisodes.

La moyenne des 100 derniers épisodes (Avg100) s’améliore progressivement de −21 à −19.2.

Le taux d’exploration (ε) diminue vers 0.37 : le modèle exploite davantage ses connaissances.

🧩 Interprétation :

Cette amélioration indique que l’agent commence à reconnaître certaines situations favorables, à suivre la balle plus longtemps, et à retarder la défaite.
Le passage de −21 → −12 montre que l’agent perd moins vite, preuve d’un apprentissage des dynamiques visuelles et temporelles grâce au couple CBAM + LSTM.

🚀 Conclusion et perspectives

✅ Résultats :

Passage de EpR = −21 → EpR = −12

Moyenne 100 derniers épisodes : −21 → −19.2

L’agent améliore sa survie et réagit mieux visuellement.

⚙️ Interprétation technique :

Le CNN+CBAM apprend à focaliser sur la balle et la raquette.

Le LSTM aide à comprendre la direction et la vitesse du mouvement.

Le DQN apprend lentement, mais les premières tendances sont claires : meilleure stabilité et réactivité.
