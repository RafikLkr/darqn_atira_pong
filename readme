ğŸ§  Deep Q-Learning with CNN + CBAM + LSTM on Pong ğŸ®
ğŸ¯ Objectif du projet

Lâ€™objectif de ce projet est de dÃ©velopper un agent intelligent capable dâ€™apprendre Ã  jouer au jeu Pong (Atari) en observant uniquement les images brutes de lâ€™Ã©cran.
Lâ€™agent utilise un rÃ©seau de neurones profond (DQN) combinÃ© Ã  :

ğŸ§© CBAM (Convolutional Block Attention Module) pour concentrer son attention sur les zones et canaux dâ€™image les plus pertinents (balle, raquette),

ğŸ” LSTM (Long Short-Term Memory) pour mÃ©moriser la dynamique temporelle du jeu (vitesse et trajectoire de la balle).

ğŸ•¹ï¸ PrÃ©sentation du jeu Pong

Pong est un jeu dâ€™arcade classique simulant une partie de tennis de table.

Le joueur contrÃ´le une raquette sur le cÃ´tÃ© gauche de lâ€™Ã©cran pour renvoyer une balle.

Le but est dâ€™empÃªcher la balle de sortir de son cÃ´tÃ© tout en essayant de marquer sur lâ€™adversaire (droite).

La rÃ©compense dans ce projet :

+1 pour un point gagnÃ©,

-1 pour un point perdu,

0 sinon.

Ce cadre est parfait pour le renforcement par apprentissage, car le but de lâ€™agent est de maximiser la somme des rÃ©compenses cumulÃ©es.

âš™ï¸ Architecture du modÃ¨le

Lâ€™architecture implÃ©mente un Deep Q-Network (DQN) enrichi par des modules dâ€™attention et une mÃ©moire temporelle :

Image (84x84x1) Ã— 4 frames consÃ©cutives
     â”‚
     â–¼
Conv2D (32 filtres, 8x8, stride 4)   â†’ DÃ©tection grossiÃ¨re des objets
Conv2D (64 filtres, 4x4, stride 2)   â†’ Motifs intermÃ©diaires
Conv2D (64 filtres, 3x3, stride 1)   â†’ DÃ©tails fins
CBAM (Channel + Spatial Attention)   â†’ Focus sur zones et canaux utiles
Flatten + Dense(256, ReLU)           â†’ Encodage visuel par frame
TimeDistributed(...) Ã— 4 frames      â†’ SÃ©quence temporelle
LSTM(128)                            â†’ Apprentissage de la dynamique (vitesse, direction)
Dense(128, ReLU)
Dense(nb_actions, Linear)            â†’ Valeurs Q pour chaque action


Perte : Huber Loss (plus stable que MSE)
Optimiseur : Adam
RÃ©seau cible : mis Ã  jour tous les 10 Ã©pisodes
StratÃ©gie : Îµ-greedy avec dÃ©croissance progressive de Îµ

ğŸ“Š RÃ©sultats dâ€™apprentissage
ğŸŸ  DÃ©but dâ€™entraÃ®nement

Ã‰pisodes 1 Ã  20 :

Les rÃ©compenses sont constantes autour de âˆ’21, correspondant Ã  une dÃ©faite immÃ©diate Ã  chaque partie.

La moyenne mobile des 100 derniers Ã©pisodes (Avg100) reste stable Ã  âˆ’21.

Lâ€™agent explore alÃ©atoirement sans encore apprendre de stratÃ©gie.

ğŸ§© InterprÃ©tation :

Ã€ ce stade, lâ€™agent joue sans logique : il ne sait ni suivre la balle, ni anticiper les rebonds.
Le rÃ©seau est en phase dâ€™exploration et le Q-learning nâ€™a pas encore convergÃ©.

ğŸŸ¢ Milieu/Fin dâ€™entraÃ®nement

Ã‰pisodes 100 Ã  120 :

Les rÃ©compenses passent de âˆ’21 Ã  environ âˆ’12 sur certains Ã©pisodes.

La moyenne des 100 derniers Ã©pisodes (Avg100) sâ€™amÃ©liore progressivement de âˆ’21 Ã  âˆ’19.2.

Le taux dâ€™exploration (Îµ) diminue vers 0.37 : le modÃ¨le exploite davantage ses connaissances.

ğŸ§© InterprÃ©tation :

Cette amÃ©lioration indique que lâ€™agent commence Ã  reconnaÃ®tre certaines situations favorables, Ã  suivre la balle plus longtemps, et Ã  retarder la dÃ©faite.
Le passage de âˆ’21 â†’ âˆ’12 montre que lâ€™agent perd moins vite, preuve dâ€™un apprentissage des dynamiques visuelles et temporelles grÃ¢ce au couple CBAM + LSTM.

ğŸš€ Conclusion et perspectives

âœ… RÃ©sultats :

Passage de EpR = âˆ’21 â†’ EpR = âˆ’12

Moyenne 100 derniers Ã©pisodes : âˆ’21 â†’ âˆ’19.2

Lâ€™agent amÃ©liore sa survie et rÃ©agit mieux visuellement.

âš™ï¸ InterprÃ©tation technique :

Le CNN+CBAM apprend Ã  focaliser sur la balle et la raquette.

Le LSTM aide Ã  comprendre la direction et la vitesse du mouvement.

Le DQN apprend lentement, mais les premiÃ¨res tendances sont claires : meilleure stabilitÃ© et rÃ©activitÃ©.
