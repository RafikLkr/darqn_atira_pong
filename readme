
### ðŸ§  DÃ©tails des composants

| Composant | RÃ´le | Pourquoi |
|------------|------|-----------|
| **Conv2D** | Extraction de motifs visuels | Permet de repÃ©rer la balle, les raquettes et les rebonds |
| **CBAM (Attention)** | Focus sur les zones importantes (balle, raquette) | RÃ©duit le bruit visuel, amÃ©liore la stabilitÃ© |
| **LSTM** | IntÃ¨gre la dynamique temporelle | Apprend la direction et la vitesse du mouvement |
| **Huber Loss** | Fonction de perte robuste | Moins sensible aux erreurs extrÃªmes que MSE |
| **Replay Buffer** | MÃ©morise les transitions passÃ©es | DÃ©corrÃ¨le les observations, amÃ©liore la convergence |
| **Target Network** | RÃ©seau cible mis Ã  jour pÃ©riodiquement | Stabilise lâ€™apprentissage DQN |
| **Îµ-Greedy Policy** | Exploration contrÃ´lÃ©e | DÃ©croÃ®t progressivement de 1 â†’ 0.01 |

---

## ðŸ“Š RÃ©sultats dâ€™apprentissage

### ðŸŸ  DÃ©but de lâ€™entraÃ®nement
![DÃ©but de lâ€™entraÃ®nement](training_beginning.png)

#### ðŸ” Analyse :
- Les 20 premiers Ã©pisodes montrent une rÃ©compense constante autour de **âˆ’21**, soit une **dÃ©faite complÃ¨te** Ã  chaque partie.
- Lâ€™agent agit **alÃ©atoirement** (phase dâ€™exploration).
- La moyenne des 100 derniers Ã©pisodes (**Avg100**) est de **âˆ’21**, signe quâ€™aucune stratÃ©gie nâ€™est encore apprise.

ðŸ§© **InterprÃ©tation :**
Le rÃ©seau commence Ã  explorer le jeu. Il ne sait pas encore oÃ¹ est la balle ni comment bouger la raquette efficacement.

---

### ðŸŸ¢ Milieu / Fin dâ€™entraÃ®nement
![Fin de lâ€™entraÃ®nement](training_end.png)

#### ðŸ” Analyse :
- Autour de lâ€™Ã©pisode **100**, la rÃ©compense instantanÃ©e (**EpR**) passe de **âˆ’21 Ã  âˆ’12**.
- La moyenne des 100 derniers Ã©pisodes (**Avg100**) progresse de **âˆ’21 Ã  âˆ’19.2**.
- Le taux dâ€™exploration (Îµ) diminue progressivement (~0.37), ce qui signifie que lâ€™agent **agit de plus en plus selon ce quâ€™il a appris**.

ðŸ§© **InterprÃ©tation :**
> Lâ€™agent commence Ã  comprendre comment prolonger les Ã©changes.
> Il **perd moins vite**, anticipe mieux les rebonds et suit la balle plus longtemps.
> La combinaison **CBAM + LSTM** aide Ã  reconnaÃ®tre **les zones importantes** et **les dynamiques temporelles**.

---

## ðŸš€ SynthÃ¨se et interprÃ©tation

| Indicateur | DÃ©but | AprÃ¨s 100 Ã©pisodes | Progression |
|-------------|--------|--------------------|--------------|
| **EpR (Reward par Ã©pisode)** | âˆ’21 | âˆ’12 | ðŸ”¼ +9 points |
| **Avg100 (Moyenne mobile)** | âˆ’21 | âˆ’19.2 | ðŸ”¼ +1.8 points |
| **Exploration Îµ** | 1.00 | 0.37 | ðŸ”½ baisse progressive |

âœ… **Ce que cela montre :**
- Lâ€™agent commence Ã  dÃ©velopper un comportement cohÃ©rent.
- Lâ€™apprentissage est **lent mais rÃ©gulier**, typique dâ€™un DQN sur environnement visuel.
- On observe une **transition claire entre exploration et exploitation**.

---

## ðŸ’¡ Perspectives dâ€™amÃ©lioration

1. **Double DQN** â†’ rÃ©duire la surestimation des Q-valeurs.
2. **Dueling Network** â†’ sÃ©parer la valeur de lâ€™Ã©tat et lâ€™avantage de chaque action.
3. **Prioritized Experience Replay** â†’ rejouer plus souvent les transitions importantes.
4. **Augmenter le nombre dâ€™Ã©pisodes** (> 5000) pour convergence complÃ¨te.
5. **Optimiseur RMSProp** (comme dans lâ€™article original de DeepMind).

---

## ðŸ“š RÃ©fÃ©rences

- Mnih et al., *"Human-level control through deep reinforcement learning"*, **Nature, 2015**
- Woo et al., *"CBAM: Convolutional Block Attention Module"*, **ECCV, 2018**
- Hochreiter & Schmidhuber, *"Long Short-Term Memory"*, **Neural Computation, 1997**

---

## ðŸ§© Exemple de logs d'entraÃ®nement